### 1概述
存储格式的选择是大数据场景下很重要的一点下面我们就对一些常用的存储格式进行分析与对比，先从官网看看Hive中到底有哪些存储格式：
```
[STORED AS file_format]
file_format:
  : SEQUENCEFILE
  | TEXTFILE    -- (Default, depending on hive.default.fileformat configuration)
  | RCFILE      -- (Note: Available in Hive 0.6.0 and later)
  | ORC         -- (Note: Available in Hive 0.11.0 and later)
  | PARQUET     -- (Note: Available in Hive 0.13.0 and later)
  | AVRO        -- (Note: Available in Hive 0.14.0 and later)
  | INPUTFORMAT input_format_classname OUTPUTFORMAT output_format_classname
```
### 2TEXTFILE：文本文件
1. 存储方式：行存储；
2. 默认格式，数据不做压缩，磁盘开销大，数据解析开销大。可结合Gzip、Bzip2使用(系统自动检查，执行查询时自动解压)，但使用这种方式，hive不会对数据进行切分，从而无法对数据进行并行操作。

```
创建一张tt表进行分析

CREATE TABLE tt (
id int,
name string
) ROW FORMAT DELIMITED FIELDS TERMINATED BY "\t";1234
```

查看tt表格信息
```
desc formatted tt
# Storage Information            
SerDe Library:          org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe       
InputFormat:            org.apache.hadoop.mapred.TextInputFormat         
OutputFormat:           org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat       
Compressed:             No                       
Num Buckets:            -1                       
Bucket Columns:         []                       
Sort Columns:           []                       
Storage Desc Params:             
        field.delim             \t                  
        serialization.format    \t 123456789101112
```
这两种方式也是创建TEXTFILE的存储格式：
```
方式一：
CREATE TABLE tt2 (
id int,
name string
) ROW FORMAT DELIMITED FIELDS TERMINATED BY "\t"
STORED AS TEXTFILE;
方式二：
CREATE TABLE tt3 (
id int,
name string
) ROW FORMAT DELIMITED FIELDS TERMINATED BY "\t"
STORED AS 
INPUTFORMAT 'org.apache.hadoop.mapred.TextInputFormat'
OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat';
```

### 3Sequencefile：二进制文件
1. 概念</br>
SequenceFile是一个由二进制序列化过的key/value的字节流组成的文本存储文件，它可以在map/reduce过程中的input/output的format时被使用。在map/reduce过程中，map处理文件的临时输出就是使用SequenceFile处理过的。 所以一般的SequenceFile均是在FileSystem中生成，供map调用的原始文件。

2. 特点</br>
存储方式：行存储；

SequenceFile是Hadoop API 提供的一种二进制文件，它将数据(key,value)的形式序列化到文件里。这样的二进制文件内部使用Hadoop 的标准的Writable 接口实现序列化和反序列化。它与Hadoop API中的MapFile 是互相兼容的。 

Hive 中的SequenceFile 继承自Hadoop API 的SequenceFile，只是它的key为空。使用value 存放实际的值。 这样是为了避免MR 在执行map 阶段的排序过程。
SEQUENCEFILE比TextFile 多了一些冗余信息，所以要大一些。

SequenceFile是Hadoop 的一个重要数据文件类型，它提供key-value的存储，但与传统key-value存储（比如hash表，btree）不同的是，它是append only的，于是你不能对已存在的key进行写操作。SequenceFile可以作为小文件的容器，可以通过它将小文件包装起来传递给MapReduce进行处理。SequenceFile提供了两种定位文件位置的方法:</br>
(1)seek(long poisitiuion):poisition必须是记录的边界，否则调用next()方法时会报错</br>
(2)sync(longpoisition):Poisition可以不是记录的边界，如果不是边界，会定位到下一个同步点，如果Poisition之后没有同步点了，会跳转到文件的结尾位置

3. 状态</br>
SequenceFile 有三种压缩态：</br>
(1)Uncompressed:未进行压缩的状  </br>
(2)record compressed:对每一条记录的value值进行了压缩（文件头中包含上使用哪种压缩算法的信息）  </br>
(3)block compressed:当数据量达到一定大小后，将停止写入进行整体压缩，整体压缩的方法是把所有的keylength,key,vlength,value分别合在一起进行整体压缩，块的压缩效率要比记录的压缩效率高</br>

4. 格式</br>
每一个SequenceFile都包含一个“头”（header)。Header包含了以下几部分。</br>
(1)SEQ三个字母的byte数组</br>
(2)Version number的byte，目前为数字3的byte</br>
(3)Key和Value的类名</br>
(4)压缩相关的信息</br>
(5)其他用户定义的元数据</br>
(6)同步标记，sync marker </br>
对于每一条记录（K-V），其内部格式根据是否压缩而不同。SequenceFile的压缩方式有两种，“记录压缩”（record compression）和“块压缩”（block compression）。如果是记录压缩，则只压缩Value的值。如果是块压缩，则将多条记录一并压缩，包括Key和Value。具体格式如下面两图所示：
![png](https://github.com/ordinary-zhang/hadoop/blob/master/hadoop/%E5%9B%BE%E7%89%87/block%20compress.png)
![png](https://github.com/ordinary-zhang/hadoop/blob/master/hadoop/%E5%9B%BE%E7%89%87/sequence.png)

创建page_views_seq表
```
create table page_views_seq(
track_time string,
url string,
session_id string,
referer string,
ip string,
end_user_id string,
city_id string
)ROW FORMAT DELIMITED FIELDS TERMINATED BY "\t"
STORED AS SEQUENCEFILE;

这时候并不能使用load将数据加载到存储格式为SEQUENCEFILE的表中，需要先创建TextFile的存储格式的表中，然后insert into到存储格式为SEQUENCEFILE的表中。

插入数据：insert into table page_views_seq select * from page_views; 
数据大小对比：
TextFile存储格式的大小
[hadoop@hadoop ~]$ hdfs dfs -du -s -h /user/hive/warehouse/hive.db/page_views
18.1 M   /user/hive/warehouse/hive.db/page_views
SEQUENCEFILE存储格式的大小
[hadoop@hadoop ~]$ hdfs dfs -du -s -h /user/hive/warehouse/hive.db/page_views_seq 
19.6 M   /user/hive/warehouse/hive.db/page_views_seq
原始大小
[hadoop@hadoop ~]$ du -sh data/page_views.dat
19M     data/page_views.dat
```
### 4Rcfile
1. Rcfile是一种列式存储方式，所以它具备以下特点：</br>
(1) RCFile通过列进行数据压缩，因为同一列都是相同的数据类型，所以压缩比比较好</br>

(2) RCFile可以跳过不必要的列读取</br>

![rc](https://github.com/ordinary-zhang/hadoop/blob/master/hadoop/%E5%9B%BE%E7%89%87/rcfile.PNG)
RCFile占用多个block,每一个block以rowgroup(行组)为单位进行组织记录，也就是说存储在一个HDFS Block块中的所有记录被划分为多个行组，对于一张表，所有行组大小相同。一个HDFS块可能有一个或者多个行组
 
2. 行组包括是三个部分：
Sync:行组头部的同步标志,主要用于隔离HDFS 块中两个连续的行组，大小为16字节。

MetadataHeader：行组的元数据头部，存储行组元数据信息，比如行组中的记录数,每一个列的字节数，列中每一个域的字节数

实际数据：存储顺序是按照域顺序存储。
 
3. 压缩方式：
RCFile的每一个行组，元数据头部和时真实数据分别被压缩。对所有元数据头部，RCFile使用RLE(RunLength Encoding )算法压缩
 
真实数据不会按照真个单元压缩，而是按照一列一列的独立压缩，使用GZip算法，可以获得较好的压缩比。
 
4. 数据追加
RCFile不支持任意方式的数据追加，因为HDFS仅仅支持文件的追加写到尾部。RCFile为每一列创建并维护一个ColumnHolder，当记录追加，所有域被分发，每一个域追加其到对应的ColumnHolder，另外RCFile在元数据头部记录每一个域对应的元数据。
 
RCFile提供两个参数来控制在刷写到磁盘之前，内存中缓存多少个记录。一个参数是记录数的限制，另一个是内存缓存的大小限制。RCFile首先压缩元数据头部并写到磁盘，然后分别压缩每个columnholder，并将压缩后的columnholder刷写到底层文件系统中的一个行组中。
 
5. 数据读取和Lazy解压
Map-Reduce，Mapper顺序处理处理每一个块中的行组，当处理一个行组的时候，RCFile无需全部读取行组的全部数据到内存。他是怎么做的呢？

它仅仅读取元数据头部和给定要查询的列。因此，他可以跳过不必要的列，比如表A(field1,field2,field3,filed4,field5)，做查询的时候：SELECTfield1 FROM A WHERE field5 = 'CN'.这样的话，对于每一个行组，他只需要读取field1和field5内容。
 
在元数据头部和需要的列数据加载到内存后，他们需要解压。元数据头部总会解压并在内存中维护直到RCFile处理下一个行组。然而，RCFile不会解压所有加载的列，它使用一种Lazy解压技术：直到RCFile决定列中数据对查询有用才会去解压，由于使用这种WHERE条件，这种解压技术显得很有用，它直接压满足条件的列

6. 创建page_views_seq表

```
create table page_views_rcfile(
track_time string,
url string,
session_id string,
referer string,
ip string,
end_user_id string,
city_id string
)ROW FORMAT DELIMITED FIELDS TERMINATED BY "\t"
STORED AS RCFILE;
插入数据：
insert into table page_views_rcfile select * from page_views;
查看大小：
hdfs dfs -du -s -h /user/hive/warehouse/hive.db/page_views_rcfile
17.9 M  /user/hive/warehouse/hive.db/page_views_rcfile
```

### 5ORC
ORCFile存储格式，就是OptimizedRC File的缩写。意指优化的RCFile存储格式。ORC具有以下一些优势: 
1. ORC是列式存储，有多种文件压缩方式，并且有着很高的压缩比。 
2. 文件是可切分（Split）的。因此，在Hive中使用ORC作为表的文件存储格式，不仅节省HDFS存储资源，查询任务的输入数据量减少，使用的MapTask也就减少了。 
3. 提供了多种索引，row group index、bloom filter index。 
4. ORC可以支持复杂的数据结构（比如Map等）
![](https://github.com/ordinary-zhang/hadoop/blob/master/hadoop/%E5%9B%BE%E7%89%87/orc.png)

ORC文件存储分为多个stripes每一个stripe又包含IndexData(索引数据),RowData(行数据)StripeFooter，每一个默认是250MB,大的好处是HDFS读的效率更高
 
 
IndexData: 保存的是每一列的最大值和最小值，以及每一列所在的行.

row_index包括了改行的偏移量以及改行的长度：</br>
Stream:column 0 section ROW_INDEX start: 3 length 11 </br>
Stream:column 1 section ROW_INDEX start: 14 length 28 </br>
所以他就可以跳到正确的压缩块位置。一个stripe中包含多个row group，默认为10000个值组成 

RowData：保存的实际数据 

StripeFooter: stream的位置信息

stream：一个stream表示文件中一段有效的数据，包括索引和数据两类。索引stream保存每一个row group的位置和统计信息，数据stream包括多种类型的数据，具体需要哪几种是由该列类型和编码方式决定。

##### 特点：
1. 读取ORC文件是从尾部开始的，第一次读取16KB的大小，尽可能的将Postscript和Footer数据都读入内存。文件的最后一个字节保存着PostScript的长度，它的长度不会超过256字节，PostScript中保存着整个文件的元数据信息，它包括文件的压缩格式、文件内部每一个压缩块的最大长度(每次分配内存的大小)、Footer长度，以及一些版本信息。在Postscript和Footer之间存储着整个文件的统计信息(上图中未画出)，这部分的统计信息包括每一个stripe中每一列的信息，主要统计成员数、最大值、最小值、是否有空值等。

2. 接下来读取文件的Footer信息，它包含了每一个stripe的长度和偏移量，该文件的schema信息(将schema树按照schema中的编号保存在数组中)、整个文件的统计信息以及每一个row group的行数。

3 处理stripe时首先从Footer中获取每一个stripe的其实位置和长度、每一个stripe的Footer数据(元数据，记录了index和data的的长度)，整个striper被分为index和data两部分，stripe内部是按照row group进行分块的(每一个row group中多少条记录在文件的Footer中存储)，row group内部按列存储。每一个row group由多个stream保存数据和索引信息。每一个stream的数据会根据该列的类型使用特定的压缩算法保存。

![](https://github.com/ordinary-zhang/hadoop/blob/master/hadoop/%E5%9B%BE%E7%89%87/%E5%8E%8B%E7%BC%A9%E6%AF%94.png)
**总结：**  ORC能很大程序的节省存储和计算资源，但它在读写时候需要消耗额外的CPU资源来压缩和解压缩，当然这部分的CPU消耗是非常少的。 
对性能提升的另一个方面是通过在ORC文件中为每一个字段建立一个轻量级的索引，来判定一个文件中是否满足WHERE子句中的过滤条件。比如：当执行HQL语句”SELECT COUNT(1) FROM lxw1234_orc WHERE id = 0”时候，先从ORC文件的metadata中读取索引信息，快速定位到id=0所在的offsets，如果从索引信息中没有发现id=0的信息，则直接跳过该文件。

##### 创建page_views_seq表
```
create table page_views_orc_zlib
ROW FORMAT DELIMITED FIELDS TERMINATED BY "\t"
STORED AS ORC 
TBLPROPERTIES("orc.compress"="ZLIB")
as select * from page_views;

[hadoop@hadoop ~]$ hdfs dfs -du -s -h /user/hive/warehouse/hive.db/page_views_orc_zlib
2.8 M  /user/hive/warehouse/hive.db/page_views_orc_zlib
```

### 6Parquet
![](https://github.com/ordinary-zhang/hadoop/blob/master/hadoop/%E5%9B%BE%E7%89%87/parquet.png)

在有些时候，比如数据文件是嵌套的。这个时候，列式存储怎么工作呢？这就引出了Parquet存储。
Parquet是不跟任何数据处理技术绑定在一起的，可以用于多种数据处理框架。

查询引擎：Hive,Imapla,Presto等

计算框架：Map-Reduce，Spark等

数据模型：Avro,Thrift,PB
 
1. Parquet如何与这些组件工作呢？

存储格式：定义了parquet内部的数据类型，存储格式

对象模型转换器：完成外部对象模型和parquet内部数据类型的映射

对象模型：Avro，Thrift等都是对象模型

Avro,Thrift, Protocol Buffers都有他们自己的存储格式，但是Parquet并没有使用他们，而是使用了自己定义的存储格式。所以如果使用了Avro等对象模型，这些数据序列化到磁盘最后使用的是parquet的转换器把他们转成parquet自己的格式。




























